{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librería wget\n",
    "\n",
    "1. Leer archivos .txt  en Python.\n",
    "\n",
    "    1.1. Uso de wget para leer archivos .txt.\n",
    "    \n",
    "    1.2. La estructura recomendada para abrir ficheros con este paquete es: \n",
    "        \n",
    "    with open({archivo con la ruta}, 'r') as {nombre de la variable donde lo almacenaremos}:\n",
    "    \n",
    "        {operaciones}\n",
    "    \n",
    "    **1.3. Las rutas se indican con / y solo hay que indicar la parte de la ruta posterior a la de nuestro notebook (la ruta que usa la función como punto de partida es la del notebook donde estamos trabajando).**\n",
    "    \n",
    "    1.4. Hay otras funciones en este documento, pero no creo que merezca la pena indagar más en ello ahora mismo.\n",
    "\n",
    "2. Escribir y leer archivos en Python.\n",
    "    \n",
    "    2.1. En este caso se usa 'w' en lugar de 'r' y el método .write({lo que queramos añadir al documento}).\n",
    "    \n",
    "    2.2. Se pueden usar comandos como \\n para hacer saltos de línea y cosas por el estilo.\n",
    "    \n",
    "    2.3. Para copiar un archivo en uno nuevo, lees el primero, en su indentación escribes sobre el nuevo (si el archivo definido no está en la carpeta del notebook te lo crea automáticamente) y al escribir usas un for haciendo referencia al primero que escriba (método .write({elemento del primero a iterar})) las líneas del primero en el segundo.\n",
    "    \n",
    "# Librería pandas\n",
    "\n",
    "3. Introducción. Cargar y filtrar datos con Pandas.\n",
    "\n",
    "    **3.1. import pandas as pd**\n",
    "    \n",
    "    **3.2. Para leer archivos .csv en pd usamos: pd.read_csv({csv_path}). Te lo lee como un pd DataFrame.**\n",
    "    \n",
    "    3.3. El método .head({n}) te permite visualizar los {n} primeros elementos del DataFrame (si no añades {n}, te muestra los 5 primeros).\n",
    "    \n",
    "    **3.4. La función type() se puede usar para comprobar el tipo del objeto creado, que en el caso anterior será un pandas.core.frame.DataFrame.**\n",
    "    \n",
    "    **3.5. Análogamente a la anterior, existe la función pd.read_excel  para leer excels (formato .xlsx).** Sin embargo, para leer excels existe una alternativa: pd.ExcelFile({xlsx_path}), aunque en este caso no lo convierte en DataFrame, sino en pandas.io.excel._base.ExcelFile, lo que da problemas para usar otras funciones de pd sobre él, como el método .head().\n",
    "    \n",
    "    3.6. Si queremos trabajar con pd:ExcelFile, hay que usar .sheet_names (**sin paréntesis**) para comprobar los nombres de las hojas y escribir {variable donde hemos guardado el excel}.parse('{nombre de una de las hojas}'), ya que así convertimos las hojas a DataFrame y ya podemos trabajar con ellas (además, .parse() te muestra los 5 primeros resultados de la hoja como DataFrame).\n",
    "    \n",
    "    **3.7. Podemos filtrar una o varias columnas del df por su nombre de la siguiente forma: {df_name}[['{col1_name}', '{col2_name}']]. El doble corchete es para mantener el tipo del resultado como DataFrame (si solo pones uno, el resultado es un pd Series).**\n",
    "    \n",
    "    **3.8. El método .iloc[{i}, {j}]** te devuelve la información de la fila con **posición** {i} y la columna de **posición** {j} de un pd DataFrame **en forma de un pd Series**. Si suprimes {j}, te devuelve toda la información de la fila de **posición** {i}\n",
    "    \n",
    "    **3.9. El método .loc[] es análogo al .iloc[], pero sustituyendo {j} por {'col_name'}** y **siendo {i} el índice del DataFrame** (puede no coincidir con la posición si ordenamos el DataFrame). Como vemos, tiene esas 2 particularidades con respecto a .iloc[].\n",
    "    \n",
    "    3.10. Tanto con .loc[] como con .iloc[] puedes hacer slicing y, **en el caso de .loc[] se hace así: {'colN_name'}:{'colM_name'}, donde M>N y {'colM_name'} está incluido** (recordemos que en N:M, M no estaría incluido). \n",
    "    \n",
    "    3.11. **Podemos usar .loc[] para aplicar condiciones también (.loc[{condición}]). Sin embargo, la parte de la condición donde están los datos que vamos a comparar debe escribirse como pd Series (con un corchete)**, porque **como pd DataFrame no funciona**. \n",
    "    \n",
    "    **3.12. No hemos visto cómo aplicar condiciones sobre columnas (punto anterior) con iloc[]__.**\n",
    "    \n",
    "    **3.13. El método .sort_values('{col_name}') te ordena los resultados del DataFrame en orden creciente según esa columna**, ya sea numérica o de strings. Sin embargo, **los índices no se reordenan**, cuidado con eso.\n",
    "    \n",
    "    **3.14. El método .index (sin paréntesis) devuelve los índices de un DataFrame en el orden en que se encuentran en el mismo** (para un DataFrame original están ordenados, pero si el DataFrame ha sido ordenado tras su creación, los índices no estarán en su posición original). \n",
    "    \n",
    "    \n",
    "4. Introducción. Parte 2.\n",
    "\n",
    "    4.1. La función **pd.read_csv** tiene un argumento llamado **header**. Si hacemos header = None no se va a tomar la primera fila del documento csv como los nombres de las columnas (que es lo que hacer por defecto).\n",
    "    \n",
    "    4.2. .tail() es una función análoga a .head(), pero te devuelve las filas del dataframe empezando por la última en lugar de por la primera.\n",
    "    \n",
    "    4.3. Una opción para **añadir nombres a las columnas de nuestro pd DataFrame** si no lo tienen, es crear una lista (recuérdese que van entre corchetes) y hacer **{DataFrame_name}.columns = {esa lista}**. Esto es, el método columns nos muestra las columnas del DataFrame y nos permite sobreescribir los nombres de las columnas.\n",
    "    \n",
    "    4.4. **df.{col_name} te devuelve un pd Series con los datos de la columna y sus índices en el DataFrame** del que provienen.\n",
    "    \n",
    "    4.5. Para inspeccionar un DataFrame podemos usar el **método .dtypes (sin paréntesis)**, que nos devuelve los tipos de las diferentes columnas del DataFrame.\n",
    "    \n",
    "    4.6. A su vez, el método **.describe() te devuelve los principales valores estadísticos de cada columna del DataFrame**. Si en el paréntesis incluimos el **argumento include = 'all', te devolverá incluso más estadísticos** (te da la versión completa de .describe()).\n",
    "    \n",
    "    4.7. El **método .info** también es útil para inspeccionar un DataFrame.\n",
    "    \n",
    "    4.8. Puedes escribir **{df_name}.isna().sum()** para obtener un pd Series con los nombres de las columnas y el número de na's en cada uno de ellas. (Es decir, **para ver si hay valores perdidos en el DataFrame**).\n",
    "    \n",
    "    4.9. Podemos **pasar un DataFrame a formato .csv y almacenarlo en la carpeta donde está nuestro Notebook (por defecto)** usando el **método .to_csv('{file_name.csv}', index = False)**, donde se suele colocar **index = False para que en el archivo guardado no aparezcan los índices del DataFrame** (que es la opción por defecto). **Para leer o guardar en otros formatos se sigue la misma lógica**:\n",
    "    \n",
    "| Data Formate  | Read           | Save             |\n",
    "| ------------- |:--------------:| ----------------:|\n",
    "| csv           | `pd.read_csv()`  |`df.to_csv()`     |\n",
    "| json          | `pd.read_json()` |`df.to_json()`    |\n",
    "| excel         | `pd.read_excel()`|`df.to_excel()`   |\n",
    "| hdf           | `pd.read_hdf()`  |`df.to_hdf()`     |\n",
    "| sql           | `pd.read_sql()`  |`df.to_sql()`     |\n",
    "| ...           |   ...          |       ...        |\n",
    "\n",
    "    como se puede observar, se sigue la misma lógica.\n",
    "    \n",
    "    \n",
    "5. Data Wrangling\n",
    "\n",
    "    5.1. **Se puede usar el argumento names = {lista con los nombres de las columnas} en .read_csv() para añadir directamente los nombres de las columnas al DataFrame** en lugar de hacer header = None y luego cambiarlos como vimos en el apartado 4.\n",
    "    \n",
    "    5.2. Por defecto, **si hay muchas columnas en un DataFrame**, pd te va a mostrar solo algunas del pricipio y del final del mismo, **para que te las muestre todas** cuando llames al df (por ejemplo, con un .head()) tienes que escribir **en una línea de código aparte pd.set_option('display.max_columns', None)**.\n",
    "    \n",
    "    5.3. El **método .replace({lo que quieres sustituir}, {lo que lo va a sustituir}, inplace = True)** sustituye una cosa por otra en el DataFrame y, **con inplace = True, te sobreescribe directamente el df usado** (no tienes que hacer df = df.replace(...).\n",
    "    \n",
    "    5.4. El **método .isnull() nos devuelve True en las posiciones del df donde había nan's y False en caso contrario**. Como en 4.9., **podemos usar .isnull().sum() para obtener el nº de nan de cada columna**. Si nos fijamos, la suma se realiza aprovechando que False = 0 y True = 1.\n",
    "    \n",
    "    5.5. El **método .value_counts()** cuenta las veces que se repite cada valor que aparece en una columna del df (si lo aplicamos sobre una, por ejemplo, {df_name}['{col_name}']) y, por lo tanto, **la función len() aplicada sobre ese resultado te da el nº de valores distintos que hay en esa columna**.\n",
    "    \n",
    "    5.6. Si añades el **argumento dropna = False en .value_counts()**, consigues que te cuente también el nº de nan's.\n",
    "    \n",
    "    5.7. El **método .nlargest({n})**, te muestra los {n} valores que más grandes (con .value_counts() serían los 5 con mayor frecuencia, esto es, que más se repiten).\n",
    "    \n",
    "    <h3 id=\"deal_missing_values\">Deal with missing data</h3>\n",
    "    <b>How to deal with missing data?</b>\n",
    "    \n",
    "    <ol>\n",
    "        <li>drop data<br>\n",
    "            a. drop the whole row<br>\n",
    "            b. drop the whole column\n",
    "        </li>\n",
    "        <li>replace data<br>\n",
    "            a. replace it by mean<br>\n",
    "            b. replace it by frequency<br>\n",
    "            c. replace it based on other functions\n",
    "        </li>\n",
    "    </ol>\n",
    "    \n",
    "    5.9. Una opción para tratar con datos perdidos es sustituirlos por la media, para ello, si son strings se recomienda pasar la columna previamente al tipo float con un .astype(\"float\") (con el string en sí no va a funcionar) y, a continuación, aplicar el método .mean() donde podemos añadir el argumento axis = 0 ó 1 (según si queremos actuar sobre las columnas (0/'index') o sobre las filas (1/'columns')).\n",
    "    \n",
    "    5.10. Además de replace(), el **método .fillna({n})** te permite sustituir los nan's de un df o alguna de sus columnas por el valor {n}.\n",
    "    \n",
    "    5.11. El **método .idxmax()**, usado tras un .value_counts() devuelve el valor más frecuente dentro de la columna sobre la que se efectúe el .value_counts().\n",
    "    \n",
    "    5.12. Otra forma de tratar con datos perdidos es eliminar las filas del df donde aparecen. El **método .dropna(subset = ['{column_name}'], axis = 0, inplace = True)** te elimina todas las filas del df en las que la columna que hemos elegido tenga un nan.\n",
    "    \n",
    "    5.13. **El método .reset_index(drop = True, inplace = True) se usa para volver a asignar los índices al df** y es muy útil cuando hemos eliminado columnas del df (ya que al hacerlo hemos eliminado también algunos de los índices y podremos observar saltos entre los índices del df, lo que no suele ser deseable una vez que se han eliminado dichas columnas).\n",
    "    \n",
    "    5.14. Las operaciones que no se pueden lanzar directamente sobre una pd Series completa, se realizan elemento a elemento usando el **método .apply( lambda {a_var_name_you_like}: {operaciones a realizar, llamandos con {a_var_name_you_like}}, axis = 1)**. Como es más fácil verlo con un ejemplo, dejo uno aquí: \n",
    "    \n",
    "    df['description'] = df.apply(lambda row: str(row['length'])+' by '+ str(row['width']), axis=1)\n",
    "    \n",
    "    Crea una columna llamada description en df; cada fila es un string que surge de concatenar los elementos de las columnas 'length' y 'width' junto con un 'by', fila a fila.\n",
    "    \n",
    "    5.15. Para **normalizar valores**, una táctica común es dividir los diferentes valores de una columna por su valor máximo (recordemos que para ello se usa el método .max()), de modo que quedan normalizados en el rango [0,1].\n",
    "    \n",
    "    5.16. **import matplotlib as plt**\n",
    "    \n",
    "    5.17. **from matplotlib import pyplot**\n",
    "    \n",
    "    5.18. **plt.pyplot.hist({a df column})** te representa un histograma para esa columna. Algunos métodos que conviene conocer son plt.pyplot.xlabel({string}) y, análogos a este, plt.pyplot.ylabel() y plt.pyplot.title().\n",
    "    \n",
    "    5.19. Si prefieres un gráfico de barras, **podemos asignar categorías a los valores numéricos continuos de una columna de la siguiente manera:  pd.cut({df['col_name']}, {bins: un np array con los valores entre los que crear los bins, por ejemplo, creado usando linspace}, labels={una lista con los nombres de los bins}, include_lowest=True)**, donde el último argumento se usa para que, si hay n+1 bins y n nombres, el más pequeño se añada al segundo de los n+1 bins.\n",
    "    \n",
    "    5.20. Para representar el gráfico de barras se usa **pyplot.bar({lista con los nombres de los bins}, {df['col_name']}.value_counts())**.\n",
    "    \n",
    "    5.21. También se puede usar el **argumento bins = {n} dentro de plt.pyplot.hist()** para limitar el número de bins del histograma a {n}.\n",
    "    \n",
    "    5.22. Al **método .sort_values()** visto en el apartado 4. **se le puede añadir un argumento para que el orden no sea creciente y se le pueden añadir más de una columna para establecer el nuevo orden**. Ejemplo: df = df.sort_values(**['make','price']**,**ascending=[False,True]**).\n",
    "    \n",
    "    5.23. Para agrupar por columnas usamos el **método .groupby('{col_name}')** y después **le tenemos que añadir algún otro método para que al agrupar solo haya un valor por fila (por ejemplo, un .max())**. Si queremos que no nos devuelva todas las columnas, sino una en concreto podemos usar .{col_name} entre el .groupby() y la función posterior (.max() en el ejemplo que puse antes).\n",
    "    \n",
    "    5.24. Usando el **método .agg()**, una lista con los nombres de las columnas y un diccionario que relacione las columnas que queremos ver tras la agrupación con la operación que queremos realizar sobre sus valores, podemos **agrupar varias columnas al mismo tiempo y obtener columnas sobre las que realicemos varias operaciones al mismo tiempo**. Un ejemplo sencillo es el siguiente: \n",
    "    \n",
    "    df_reduced.groupby(['make','body-style']).agg({'price': 'mean', 'horsepower': 'max'})\n",
    "    \n",
    "    5.25. El **método .first()** te da el primer valor que se obtendría de ordenar en orden creciente una serie de valores (numéricos o no numéricos). REV.\n",
    "    \n",
    "    5.26. **El método .size() se puede usar con groupby() para obtener** el nº de valores que había **para cada categoría** que se ha agrupado (esto es, **la frecuencia** con que ese valor aparecía en el df original).\n",
    "    \n",
    "    5.27. Usar reset_index() tras un .groupby puede ser un buen hábito, ya que estamos obteniendo una tabla nueva y al hacer el groupby se pierden de vista los índices al mostrar el resultado.\n",
    "    \n",
    "    5.28. El **método {df_left_name}.merge({df_right_name}, on = {col_nexo_name}, how = {left/right/inner/outer.}, suffixes = ({None/algo},{None/algo}))** es el JOIN de pd. El argumento suffixes se usa si hay una variable que va a tener el mismo nombre en ambos df y queremos añadir algo a una de ellas (en este caso como un sufijo) para que se diferencien. Si no me equivoco, por defecto es inner.\n",
    "    \n",
    "      5.28.1. Podría ocurrir que la columna usada como nexo no se llame igual en ambos df, en cuyo caso hay que sustituir on = por los **argumentos left_on = y right_on =**, que funcionan exactamente igual que el on, pero te permiten resaltar ese detalle.\n",
    "    \n",
    "    5.29. El **método .rename(columns = {diccionario}, inplace = True)** permite renombrar columnas de un df. El diccionario debe asociar los nombres actuales de las columnas con los nombres por los que se quieren sustituir (en ese orden).\n",
    "    \n",
    "    5.30. Puedes usar el **método .concat([{df1},{df2}], axis = 1) para unir dos DataFrames**.\n",
    "    \n",
    "    5.31. El **método .get_dummies({df['col_name']}** te crea una columna por cada categoría/valor de la columna elegida, siendo el valor para cada fila = 1 si en esa fila el valor original era el nombre de esa columna ó 0 si no lo era.\n",
    "    \n",
    "    \n",
    "6. Análisis de datos con Python.\n",
    "\n",
    "    6.1. El método .corr() te devuelve la matriz de correlación de las variables del df.\n",
    "    \n",
    "    6.2. La función sns.regplot(x=, y=, data=) te devuelve un gráfico de dispersión con la recta de regresión y el intervalo de confianza. Después de usarla puedes usar plt.ylim({n},{m}) para asignar los límites del eje Y en la gráfica (análogo para el eje X).\n",
    "    \n",
    "    6.3. El método .describe(), por defecto, evita las variables tipo objeto. Si queremos que ver sus estadísticos solo tenemos que añadir el argumento include=['object'] dentro del paréntesis. Las variables tipo objeto también tienen estadísticos distintos a los de las variables numéricas, como era de esperar.\n",
    "    \n",
    "    6.4. Podemos convertir un pd Series a un pd df usando el método .to_frame().\n",
    "    \n",
    "    6.5. Si al nombre de un df le añades .index.name, estarás haciendo referencia al nombre de los índices.\n",
    "    \n",
    "    6.6. El argumento __as_index=False__ en .groupby() hace que la variable por la que se agrupa no constituya los índices y asigna índices nuevos para la nueva tabla (como si usáramos el reset_index después, pero sin tener que hacerlo).\n",
    "    \n",
    "    6.7. Un ejemplo de un mapa de calor hecho a partir de una matriz creada mediante .pivot(): \n",
    "    \n",
    "    plt.pcolor(grouped_pivot, cmap='RdBu') # cmap se iguala a la paleta de colores a usar.\n",
    "    \n",
    "    plt.colorbar() # Añade la barra de colores.\n",
    "    \n",
    "    plt.show() # Muestra el gráfico.\n",
    "    \n",
    "    6.8. Otro ejemplo de mapa de calor más currado:\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    im = ax.pcolor(grouped_pivot, cmap='RdBu')\n",
    "    \n",
    "    \n",
    "    #label names\n",
    "    \n",
    "    row_labels = grouped_pivot.columns.levels[1]\n",
    "    \n",
    "    col_labels = grouped_pivot.index\n",
    "    \n",
    "    \n",
    "    #move ticks and labels to the center\n",
    "    \n",
    "    ax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)\n",
    "    \n",
    "    ax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)\n",
    "    \n",
    "    \n",
    "    #insert labels\n",
    "    \n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    \n",
    "    ax.set_yticklabels(col_labels, minor=False)\n",
    "    \n",
    "    \n",
    "    #rotate label if too long\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    \n",
    "    fig.colorbar(im)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    6.9. Correlación y causalidad. Para trabajar con p-value escribimos: from scipy import stats. Ejemplo del cálculo del coef. de corr. de Pearson y el p-value asociado: pearson_coef, p_value = stats.pearsonr(df['engine-size'], df['price']).\n",
    "    \n",
    "    6.10. Uso del método .get_group('{valor de una columna por el que agrupar}')['{columna de valores agrupados}'].\n",
    "    \n",
    "    6.11. Ejemplo de obtención de los parámetros de un ANOVA: f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  \n",
    " \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
