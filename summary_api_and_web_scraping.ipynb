{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. APIs\n",
    "\n",
    "    1.0. __import requests__\n",
    "\n",
    "    1.1.1. __Uso de requests.get() para hacer una consulta__. Otras opciones son put (actualizar datos), post (exportar datos al servidor) y delete (para eliminar un registro). \n",
    "    \n",
    "    1.1.2. __Uso de .status_code__ para asegurar que no ha habido problemas conectándose a la web (tiene que devolver 200)\n",
    "\n",
    "    1.2.1. __Uso de .json()__ para mostrar los datos en forma de Json.\n",
    "    \n",
    "    1.2.2. Ejemplo de extracción de un valor que está en el segundo nivel del json importado: response_data['rates']['AUD']. Como se puede observar, se usa la notación de varios corchetes, uno por cada nivel en el que filtramos.\n",
    "\n",
    "    1.3. Uso del string query o de un objeto externo (concretamente un diccionario) para filtrar datos. Ejemplo: \n",
    "    \n",
    "        resp = requests.get('https://api.exchangeratesapi.io/latest?base=GBP')\n",
    "    \n",
    "    Aquí se puede observar el string query en la url, con diccionario dejaríamos el latest y escribiríamos:\n",
    "    \n",
    "        payload = {'base':'GBP'}\n",
    "    \n",
    "        resp = requests.get('https://api.exchangeratesapi.io/latest', params = payload)\n",
    "\n",
    "    1.4. Uso del endpoint (latest, history, etc.) según lo que queramos hacer con los datos (mostrar los últimos datos, un histórico, etc. Respectivamente). Los endpoints dependen de la API y pueden no coincidir.\n",
    "\n",
    "    1.5. Cada vez que usamos una API nueva hay que mirar su documentación, porque sus parámetros de string query cambian con respecto a otras APIs.\n",
    "    \n",
    "\n",
    "2. Web Scraping\n",
    "\n",
    "    2.1. Consiste en extraer datos de una página web mediante robots.\n",
    "    \n",
    "    2.2. import requests\n",
    "    \n",
    "    2.3. import bs4 # Para poder comprobar la versión hay que importar el paquete entero. Aunque realmente no nos interesa, es por comprobar la versión. Usamos la 4.9.1.\n",
    "\n",
    "    2.4. from bs4 import BeautifulSoup\n",
    "    \n",
    "    2.5. De nuevo, empezamos por un {var_name} = requests.get({url}, headers={headers}), donde {headers} se usa para que la web no te reconozca como un bot (riesgo de baneo y de que no nos den los datos) y, por lo general, podemos ponerle: \n",
    "    \n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    2.6. Con .content podemos observar toda la información obtenida, pero nos interesa filtrarla, así que lo que hacemos es aplicar la función BeautifulSoup({var_name}.content, 'html.parser'), donde 'html.parser' es porque nosotros las webs están escritas en html y, por lo tanto, hay que decirle que el código que estamos extrayendo está en ese formato. Ahora tenemos una variable llamada {soup_var_name}.\n",
    "    \n",
    "    2.7. Inspeccionamos los objetos de la web que nos interesan en busca de patrones de código para esos elementos.\n",
    "    \n",
    "    2.8. Usamos el método .find_all('{tab}', class_ = '{una clase}') sobre {soup_var_name}, de manera que filtramos solo los resultados que contengan esa tab y esa clase. Almacenamos esto en una nueva variable llamada {un nombre que te guste}.\n",
    "    \n",
    "    2.9. Aplicamos el método .get_text() sobre {un nombre que te guste} para eliminar los tabs del resultado, de modo que los datos se vean más claramente. Esto se puede poner justo a continuación del .find_all.\n",
    "    \n",
    "    2.10. Otro método muy importante que es análogo a .find_all() es .find(). Funciona igual en cuanto a parámetros, pero solo escanea el primer objeto que encuentra con ese tab y clase (.find_all() examina el html completo) . Puede que te permita evitar el método .children.\n",
    "    \n",
    "    2.11. El método .children también ha sido útil alguna vez, cuando hemos querido crear un iterable para separar los datos extraídos de la web con la sopa (ese iterable se pasa a formato lista para poder obtener luego la lista de valores ya separaditos, no se muestran directamente en el iterable).\n",
    "    \n",
    "    ### Scraping de tablas\n",
    "    \n",
    "    2.12. Es parecido al normal, pero por la forma en que están estructuradas las tablas, es necesario aplicar un par de bucles for para obtener los elementos de las tablas por separado y de forma apropiada. Conviene revisarlo directamente y practicarlo.\n",
    "    \n",
    "------------------------------------------------------------------------------------------------------------------------------   \n",
    "    \n",
    " 4 ptos de Bases de Datos (sin y con Web Scraping), modificarlas, unirlas, etc.\n",
    " \n",
    " 4 ptos de numpy modificando cosas en matrices\n",
    " \n",
    " y 3 de API y avisos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
